{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09d34783",
   "metadata": {},
   "source": [
    "### Deepseek model with Langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623ed5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop-in replacement to get full logging by changing only the import\n",
    "from langfuse.openai import OpenAI\n",
    "\n",
    "# Configure the OpenAI client to use https://openrouter.ai/api/v1 as base url\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=\"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\",\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek/deepseek-chat\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is Langfuse?\"}],\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69cb76ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Langfuse** is an **open-source observability and analytics platform** designed for **LLM (Large Language Model) applications**. It helps developers and teams track, analyze, and optimize interactions with AI models by providing detailed insights into prompts, responses, costs, latencies, and user feedback.\\n\\n### **Key Features of Langfuse:**\\n1. **Prompt & Response Tracking**  \\n   - Logs full LLM interactions (input prompts, outputs, metadata).\\n   - Supports multiple model providers (OpenAI, Anthropic, Mistral, etc.).\\n\\n2. **Tracing & Debugging**  \\n   - Visualizes complex LLM workflows (e.g., multi-step chains, RAG pipelines).\\n   - Helps identify errors, inefficiencies, or unexpected behavior.\\n\\n3. **Analytics & Metrics**  \\n   - Tracks **costs, latencies, token usage**, and user feedback.\\n   - Provides dashboards for performance monitoring.\\n\\n4. **Feedback & Evaluation**  \\n   - Collects user ratings (thumbs up/down) or custom metrics.\\n   - Enables human or automated evaluation of responses.\\n\\n5. **Open-Source & Self-Hostable**  \\n   - Can be deployed on your own infrastructure (e.g., AWS, GCP).\\n   - Also offers a managed cloud version.\\n\\n6. **Integrations**  \\n   - Works with LangChain, LlamaIndex, and custom LLM applications.\\n   - Supports Python, JS/TS, and API-based logging.\\n\\n### **Use Cases:**\\n- Debugging and improving LLM applications.\\n- Optimizing cost and latency in production.\\n- Gathering user feedback for fine-tuning.\\n- Monitoring AI agent workflows.\\n\\n### **Getting Started:**\\n- **Website**: [langfuse.com](https://langfuse.com)  \\n- **GitHub**: [github.com/langfuse](https://github.com/langfuse)  \\n\\nLangfuse is particularly useful for teams building AI-powered apps who need visibility into how their models perform in real-world scenarios. Would you like help setting it up or exploring specific features?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langfuse import observe\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=\"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\"\n",
    ")\n",
    "\n",
    "@observe()\n",
    "def story():\n",
    "    return (\n",
    "        client.chat.completions.create(\n",
    "            model=\"deepseek/deepseek-chat\",\n",
    "            messages=[{\"role\": \"user\", \"content\": \"What is Langfuse?\"}],\n",
    "        )\n",
    "        .choices[0]\n",
    "        .message.content\n",
    "    )\n",
    "\n",
    "@observe()\n",
    "def main():\n",
    "    return story()\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed13b9c0",
   "metadata": {},
   "source": [
    "### Evaluate Langfuse LLM Traces with an External Evaluation Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b7f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langfuse openai deepeval --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e228546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-d34a5339-8fc2-4cdd-b91b-828c2d9447dd\" \n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-d1562c7b-7525-4835-80d0-31e153dcd175\" \n",
    "LANGFUSE_HOST=\"http://localhost:3000\"\n",
    "\n",
    "# Your openai key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879abf56",
   "metadata": {},
   "source": [
    "Let‚Äôs go ahead and generate a list of topic suggestions that we can later query to our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e73509b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'No endpoints found for deepseek/deepseek-chat:free.', 'code': 404}, 'user_id': 'user_2zXO05UXFvcyo7gqwuBgsDROSDE'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      3\u001b[39m client = OpenAI(\n\u001b[32m      4\u001b[39m     base_url=\u001b[33m\"\u001b[39m\u001b[33mhttps://openrouter.ai/api/v1\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     api_key=os.environ[\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      6\u001b[39m )\n\u001b[32m      9\u001b[39m topic_suggestion = \u001b[33m\"\"\"\u001b[39m\u001b[33m You\u001b[39m\u001b[33m'\u001b[39m\u001b[33mre a world-class journalist, specialized\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[33min figuring out which are the topics that excite people the most.\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[33mYour task is to give me 50 suggestions for pop-science topics that the general\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \u001b[33mThe use of numbers is FORBIDDEN.\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m output = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopic_suggestion\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdeepseek/deepseek-chat:free\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     27\u001b[39m \u001b[43m)\u001b[49m.choices[\u001b[32m0\u001b[39m].message.content\n\u001b[32m     29\u001b[39m topics = [item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m output.split(\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m topics:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\conda\\envs\\langfuse\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\conda\\envs\\langfuse\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1087\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1044\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1045\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1046\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1084\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1085\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1086\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1087\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1089\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1091\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1094\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1096\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1097\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1098\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1099\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1100\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1101\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1103\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1104\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1105\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1113\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1115\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1117\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1118\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1119\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1120\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1121\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1122\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1123\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1124\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1128\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\conda\\envs\\langfuse\\Lib\\site-packages\\openai\\_base_client.py:1249\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1235\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1236\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1237\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1244\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1245\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1246\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1247\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1248\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\conda\\envs\\langfuse\\Lib\\site-packages\\openai\\_base_client.py:1037\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1034\u001b[39m             err.response.read()\n\u001b[32m   1036\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1041\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNotFoundError\u001b[39m: Error code: 404 - {'error': {'message': 'No endpoints found for deepseek/deepseek-chat:free.', 'code': 404}, 'user_id': 'user_2zXO05UXFvcyo7gqwuBgsDROSDE'}"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    " \n",
    "topic_suggestion = \"\"\" You're a world-class journalist, specialized\n",
    "in figuring out which are the topics that excite people the most.\n",
    "Your task is to give me 50 suggestions for pop-science topics that the general\n",
    "public would love to read about. Make sure topics don't repeat.\n",
    "The output must be a comma-separated list. Generate the list and NOTHING else.\n",
    "The use of numbers is FORBIDDEN.\n",
    "\"\"\"\n",
    " \n",
    "output = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": topic_suggestion\n",
    "        }\n",
    "    ],\n",
    "    model=\"deepseek/deepseek-chat:free\",\n",
    " \n",
    "    temperature=1\n",
    ").choices[0].message.content\n",
    " \n",
    "topics = [item.strip() for item in output.split(\",\")]\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f308d03",
   "metadata": {},
   "source": [
    "we‚Äôll use Langfuse‚Äôs @observe() decorator. This decorator automatically monitors all LLM calls (generations) nested in the function. We‚Äôre also using the langfuse class to label and tag the traces, making it easier to fetch them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255078b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import observe, get_client\n",
    "langfuse = get_client()\n",
    " \n",
    "prompt_template = \"\"\"\n",
    "You're an expert science communicator, able to explain complex topics in an\n",
    "approachable manner. Your task is to respond to the questions of users in an\n",
    "engaging, informative, and friendly way. Stay factual, and refrain from using\n",
    "jargon. Your answer should be 4 sentences at max.\n",
    "Remember, keep it ENGAGING and FUN!\n",
    " \n",
    "Question: {question}\n",
    "\"\"\"\n",
    " \n",
    "@observe()\n",
    "def explain_concept(topic):\n",
    "    langfuse.update_current_trace(\n",
    "        name=f\"Explanation '{topic}'\",\n",
    "        tags=[\"ext_eval_pipelines\"]\n",
    "    )\n",
    "    prompt = prompt_template.format(question=topic)\n",
    " \n",
    " \n",
    "    return client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"deepseek/deepseek-chat:free\",\n",
    " \n",
    "        temperature=0.6\n",
    "    ).choices[0].message.content\n",
    " \n",
    " \n",
    "for topic in topics:\n",
    "    print(f\"Input: Please explain to me {topic.lower()}\")\n",
    "    print(f\"Answer: {explain_concept(topic)} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c35fd75",
   "metadata": {},
   "source": [
    "### 1. Fetch Your Traces\n",
    "\n",
    " We‚Äôll take an incremental approach: first, we‚Äôll fetch the initial 10 traces and evaluate them. After that, we‚Äôll add our scores back into Langfuse and move on to the next batch of 10 traces. We‚Äôll keep this cycle going until we‚Äôve processed a total of 50 traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8180cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import get_client\n",
    "from datetime import datetime, timedelta\n",
    " \n",
    "BATCH_SIZE = 10\n",
    "TOTAL_TRACES = 50\n",
    " \n",
    "langfuse = get_client()\n",
    " \n",
    "now = datetime.now()\n",
    "five_am_today = datetime(now.year, now.month, now.day, 5, 0)\n",
    "five_am_yesterday = five_am_today - timedelta(days=1)\n",
    " \n",
    "traces_batch = langfuse.api.trace.list(page=1,\n",
    "                                     limit=BATCH_SIZE,\n",
    "                                     tags=\"ext_eval_pipelines\",\n",
    "                                     from_timestamp=five_am_yesterday,\n",
    "                                     to_timestamp=datetime.now()\n",
    "                                   ).data\n",
    " \n",
    "print(f\"Traces in first batch: {len(traces_batch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76349133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content:  Ah, decision-making‚Äîour brain‚Äôs daily game of ‚Äúchoose your own adventure‚Äù! It‚Äôs a mix of logic, emotions, and even a pinch of gut feeling, all working together to guide us. Sometimes, we overthink (hello, analysis paralysis!), while other times, we go with snap judgments (thanks, instincts!). Understanding this process helps us make better choices and maybe even outsmart our own biases! üß†‚ú®\n"
     ]
    }
   ],
   "source": [
    "print(\"content: \", traces_batch[1].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe9db0f",
   "metadata": {},
   "source": [
    "### 2. Run your evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0768bb",
   "metadata": {},
   "source": [
    "#### 2.1. Categoric Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb36c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_tone_eval = \"\"\"\n",
    "You're an expert in human emotional intelligence. You can identify with ease the\n",
    " tone in human-written text. Your task is to identify the tones present in a\n",
    " piece of <text/> with precission. Your output is a comma separated list of three\n",
    " tones. PRINT THE LIST ALONE, NOTHING ELSE.\n",
    " \n",
    "<possible_tones>\n",
    "neutral, confident, joyful, optimistic, friendly, urgent, analytical, respectful\n",
    "</possible_tones>\n",
    " \n",
    "<example_1>\n",
    "Input: Citizen science plays a crucial role in research by involving everyday\n",
    "people in scientific projects. This collaboration allows researchers to collect\n",
    "vast amounts of data that would be impossible to gather on their own. Citizen\n",
    "scientists contribute valuable observations and insights that can lead to new\n",
    "discoveries and advancements in various fields. By participating in citizen\n",
    "science projects, individuals can actively contribute to scientific research\n",
    "and make a meaningful impact on our understanding of the world around us.\n",
    " \n",
    "Output: respectful,optimistic,confident\n",
    "</example_1>\n",
    " \n",
    "<example_2>\n",
    "Input: Bionics is a field that combines biology and engineering to create\n",
    "devices that can enhance human abilities. By merging humans and machines,\n",
    "bionics aims to improve quality of life for individuals with disabilities\n",
    "or enhance performance for others. These technologies often mimic natural\n",
    "processes in the body to create seamless integration. Overall, bionics holds\n",
    "great potential for revolutionizing healthcare and technology in the future.\n",
    " \n",
    "Output: optimistic,confident,analytical\n",
    "</example_2>\n",
    " \n",
    "<example_3>\n",
    "Input: Social media can have both positive and negative impacts on mental\n",
    "health. On the positive side, it can help people connect, share experiences,\n",
    "and find support. However, excessive use of social media can also lead to\n",
    "feelings of inadequacy, loneliness, and anxiety. It's important to find a\n",
    "balance and be mindful of how social media affects your mental well-being.\n",
    "Remember, it's okay to take breaks and prioritize your mental health.\n",
    " \n",
    "Output: friendly,neutral,respectful\n",
    "</example_3>\n",
    " \n",
    "<text>\n",
    "{text}\n",
    "</text>\n",
    "\"\"\"\n",
    " \n",
    " \n",
    "test_tone_score = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": template_tone_eval.format(\n",
    "                text=traces_batch[1].output),\n",
    "        }\n",
    "    ],\n",
    "    model=\"deepseek/deepseek-chat:free\",\n",
    " \n",
    "    temperature=0\n",
    ").choices[0].message.content\n",
    "print(f\"User query: {traces_batch[1].input['args'][0]}\")\n",
    "print(f\"Model answer: {traces_batch[1].output}\")\n",
    "print(f\"Dominant tones: {test_tone_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a69a5b0",
   "metadata": {},
   "source": [
    "Identifying human intents and tones can be tricky for language models. To handle this, we used a multi-shot prompt, which means giving the model several examples to learn from. Now let‚Äôs wrap our code in an evaluation function for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0513b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tone_score(trace):\n",
    "    return client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": template_tone_eval.format(text=trace.output),\n",
    "            }\n",
    "        ],\n",
    "        model=\"deepseek/deepseek-chat:free\",\n",
    "        temperature=0\n",
    "    ).choices[0].message.content\n",
    " \n",
    "tone_score(traces_batch[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aba712",
   "metadata": {},
   "source": [
    "#### 2.2. Numeric Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973d2f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1c3444",
   "metadata": {},
   "source": [
    "GEval ÊîØÊåÅÁöÑÊ®°Âûã: ÈªòËÆ§‰ΩøÁî® OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51f26d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams, LLMTestCase\n",
    " \n",
    "def joyfulness_score(trace):\n",
    "\t\tjoyfulness_metric = GEval(\n",
    "\t\t    name=\"Correctness\",\n",
    "\t\t    criteria=\"Determine whether the output is engaging and fun.\",\n",
    "\t\t    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "\t\t)\n",
    "\t\ttest_case = LLMTestCase(\n",
    "    input=trace.input[\"args\"],\n",
    "    actual_output=trace.output)\n",
    " \n",
    "\t\tjoyfulness_metric.measure(test_case)\n",
    " \n",
    "\t\tprint(f\"Score: {joyfulness_metric.score}\")\n",
    "\t\tprint(f\"Reason: {joyfulness_metric.reason}\")\n",
    " \n",
    "\t\treturn {\"score\": joyfulness_metric.score, \"reason\": joyfulness_metric.reason}\n",
    " \n",
    "joyfulness_score(traces_batch[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ad8bca",
   "metadata": {},
   "source": [
    "OpenRouter Âíå DeepseekÔºå‰øÆÊîπÊàê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bc3290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.models import DeepEvalBaseLLM\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams, LLMTestCase\n",
    "import os\n",
    "\n",
    "class CustomLLM(DeepEvalBaseLLM):\n",
    "    def __init__(self, model_name=\"deepseek/deepseek-chat\"):\n",
    "        self.model_name = model_name\n",
    "        self.client = OpenAI(\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "            api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    "        )\n",
    "    \n",
    "    def load_model(self):\n",
    "        return self.model_name\n",
    "    \n",
    "    def generate(self, prompt: str) -> str:\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "    \n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        return self.generate(prompt)\n",
    "    \n",
    "    def get_model_name(self):\n",
    "        return self.model_name\n",
    "\n",
    "# ‰ΩøÁî®Ëá™ÂÆö‰πâÊ®°ÂûãËøõË°åËØÑ‰º∞\n",
    "def joyfulness_score(trace):\n",
    "    custom_llm = CustomLLM(\"deepseek/deepseek-chat\")\n",
    "    \n",
    "    joyfulness_metric = GEval(\n",
    "        name=\"Joyfulness\",\n",
    "        criteria=\"Determine whether the output is engaging and fun.\",\n",
    "        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "        model=custom_llm\n",
    "    )\n",
    "    \n",
    "    test_case = LLMTestCase(\n",
    "        input=trace.input[\"args\"],\n",
    "        actual_output=trace.output\n",
    "    )\n",
    "    \n",
    "    joyfulness_metric.measure(test_case)\n",
    "    \n",
    "    return {\"score\": joyfulness_metric.score, \"reason\": joyfulness_metric.reason}\n",
    "\n",
    "joyfulness_score(traces_batch[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dda385",
   "metadata": {},
   "source": [
    " GEval uses chain of thought (CoT) prompting to formulate a set of criteria for scoring prompts. When developing your own metrics, it‚Äôs important to review the reasoning behind these scores. This helps ensure that the model evaluates the traces just as you intended when you wrote the evaluation prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d538dd",
   "metadata": {},
   "source": [
    "### 3. Pushing Scores to Langfuse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4964135b",
   "metadata": {},
   "source": [
    "Now that we have our evaluation functions ready, it‚Äôs time to put them to work. Use the Langfuse client to add scores to existing traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7634f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse.create_score(\n",
    "    trace_id=traces_batch[1].id,\n",
    "    name=\"tone\",\n",
    "    value=joyfulness_score(traces_batch[1])[\"score\"],\n",
    "    comment=joyfulness_score(traces_batch[1])[\"reason\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba48ad3",
   "metadata": {},
   "source": [
    "And thus, you‚Äôve added your first externally-evaluated score to Langfuse! Just 49 more to go üòÅ. But don‚Äôt worry ‚Äî our solutions are easy to scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c314ea9",
   "metadata": {},
   "source": [
    "### 4. Putting everything together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43d3b178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 processed üöÄ \n",
      "\n",
      "Batch 2 processed üöÄ \n",
      "\n",
      "Batch 3 processed üöÄ \n",
      "\n",
      "Batch 4 processed üöÄ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    " \n",
    "for page_number in range(1, math.ceil(TOTAL_TRACES/BATCH_SIZE)):\n",
    " \n",
    "    traces_batch = langfuse.api.trace.list(\n",
    "        tags=\"ext_eval_pipelines\",\n",
    "        page=page_number,\n",
    "        from_timestamp=five_am_yesterday,\n",
    "        to_timestamp=five_am_today,\n",
    "        limit=BATCH_SIZE\n",
    "    ).data\n",
    " \n",
    "    for trace in traces_batch:\n",
    "        print(f\"Processing {trace.name}\")\n",
    " \n",
    "        if trace.output is None:\n",
    "            print(f\"Warning: \\n Trace {trace.name} had no generated output, \\\n",
    "            it was skipped\")\n",
    "            continue\n",
    " \n",
    "        langfuse.create_score(\n",
    "            trace_id=trace.id,\n",
    "            name=\"tone\",\n",
    "            value=tone_score(trace)\n",
    "        )\n",
    " \n",
    "        jscore = joyfulness_score(trace)\n",
    "        langfuse.create_score(\n",
    "            trace_id=trace.id,\n",
    "            name=\"joyfulness\",\n",
    "            value=jscore[\"score\"],\n",
    "            comment=jscore[\"reason\"]\n",
    "        )\n",
    " \n",
    "    print(f\"Batch {page_number} processed üöÄ \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langfuse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
