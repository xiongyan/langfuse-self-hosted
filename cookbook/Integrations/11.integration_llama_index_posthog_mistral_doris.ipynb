{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "942a1b13",
   "metadata": {},
   "source": [
    "## [Analyze and Debug LlamaIndex Applications with PostHog and Langfuse ](https://langfuse.com/guides/cookbook/integration_llama_index_posthog_mistral)\n",
    "\n",
    "Build a RAG application with LlamaIndex, observe the steps with Langfuse, and analyze the data in PostHog.\n",
    "\n",
    "What is LlamaIndex?\n",
    "\n",
    "LlamaIndex (GitHub) is a data framework designed to connect LLMs with external data sources. It helps structure, index, and query data effectively. This makes it easier for developers to build advanced LLM applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3916e134",
   "metadata": {},
   "source": [
    "### Build a Simple RAG Application with LlamaIndex and Mistral\n",
    "\n",
    "Create a chat application that provides answers to questions about hedgehog care. LlamaIndex is used to vectorize a hedgehog care guide with the Mistral 8x22B model. All model generations are then traced using Langfuseâ€™s LlamaIndex integration.\n",
    "\n",
    "Finally, the [PostHog integration](https://langfuse.com/integrations/analytics/posthog) allows you to view detailed analytics about your hedgehog application directly in PostHog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15edb11f",
   "metadata": {},
   "source": [
    "#### Step 1: Set Up LlamaIndex and Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e288d757",
   "metadata": {},
   "source": [
    "First, we set our Mistral API key as an environment variable. If you havenâ€™t already, [sign up for a Mistral account](https://console.mistral.ai/). Then subscribe to a free trial or billing plan, after which youâ€™ll be able to generate an API key (ðŸ’¡ You can use any other model supported by LlamaIndex; we just use Mistral in this cookbook).\n",
    "\n",
    "Then, we use LlamaIndex to initialize both a Mistral language model and an embedding model. We then set these models in the LlamaIndex Settings object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dfa6cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: llama-index in e:\\conda\\envs\\langfuse\\lib\\site-packages (0.13.5)\n",
      "Collecting llama-index\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/5a/0b/62af20e8c5ac4d7608d15ee4f812b9fa88daa0c477ee64adb18674563d87/llama_index-0.13.6-py3-none-any.whl (7.0 kB)\n",
      "Collecting llama-index-llms-mistralai\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b3/46/20bf59de219a849a3e3126d4412beae7fc550509426ab9651d789c4d5f64/llama_index_llms_mistralai-0.7.0-py3-none-any.whl (8.6 kB)\n",
      "Collecting llama-index-embeddings-mistralai\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c0/05/37c7156b980d3263d80375258359682bda4123bd22a2ff559c76d8dd90cf/llama_index_embeddings_mistralai-0.4.0-py3-none-any.whl (3.4 kB)\n",
      "Requirement already satisfied: nest_asyncio in e:\\conda\\envs\\langfuse\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: llama-index-cli<0.6,>=0.5.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index) (0.5.0)\n",
      "Collecting llama-index-core<0.14,>=0.13.6 (from llama-index)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/35/23/7e497216ece6e041c6a271f2b7952e5609729da0dcdf09dd3f25a4efc1b9/llama_index_core-0.13.6-py3-none-any.whl (7.6 MB)\n",
      "     ---------------------------------------- 0.0/7.6 MB ? eta -:--:--\n",
      "     --------- ------------------------------ 1.8/7.6 MB 10.1 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 2.6/7.6 MB 8.4 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 3.9/7.6 MB 6.7 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 4.5/7.6 MB 5.3 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 6.6/7.6 MB 6.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 7.1/7.6 MB 6.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 7.1/7.6 MB 6.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 7.6/7.6 MB 4.9 MB/s  0:00:01\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.6,>=0.5.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index) (0.5.0)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index) (0.9.3)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.6,>=0.5.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index) (0.5.4)\n",
      "Requirement already satisfied: llama-index-readers-file<0.6,>=0.5.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index) (0.5.2)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index) (0.5.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-core<0.14,>=0.13.6->llama-index) (3.12.15)\n",
      "Requirement already satisfied: aiosqlite in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-core<0.14,>=0.13.6->llama-index) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-core<0.14,>=0.13.6->llama-index) (2.2.0)\n",
      "Requirement already satisfied: dataclasses-json in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-core<0.14,>=0.13.6->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-core<0.14,>=0.13.6->llama-index) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-core<0.14,>=0.13.6->llama-index) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-core<0.14,>=0.13.6->llama-index) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-core<0.14,>=0.13.6->llama-index) (2025.9.0)\n",
      "Requirement already satisfied: httpx in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-core<0.14,>=0.13.6->llama-index) (0.28.1)\n",
      "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-core<0.14,>=0.13.6->llama-index) (1.3.0)\n",
      "Requirement already satisfied: networkx>=3.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-core<0.14,>=0.13.6->llama-index) (3.5)\n",
      "Requirement already satisfied: numpy in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-core<0.14,>=0.13.6->llama-index) (2.3.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-core<0.14,>=0.13.6->llama-index) (11.3.0)\n",
      "Requirement already satisfied: platformdirs in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-core<0.14,>=0.13.6->llama-index) (4.4.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-core<0.14,>=0.13.6->llama-index) (2.11.7)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-core<0.14,>=0.13.6->llama-index) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-core<0.14,>=0.13.6->llama-index) (2.32.5)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-core<0.14,>=0.13.6->llama-index) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.6->llama-index) (2.0.43)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-core<0.14,>=0.13.6->llama-index) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-core<0.14,>=0.13.6->llama-index) (0.11.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-core<0.14,>=0.13.6->llama-index) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-core<0.14,>=0.13.6->llama-index) (4.15.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-core<0.14,>=0.13.6->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-core<0.14,>=0.13.6->llama-index) (1.17.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.6->llama-index) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.6->llama-index) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.6->llama-index) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.6->llama-index) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.6->llama-index) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.6->llama-index) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.6->llama-index) (1.20.1)\n",
      "Requirement already satisfied: griffe in e:\\conda\\envs\\langfuse\\lib\\site-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.6->llama-index) (1.13.0)\n",
      "Requirement already satisfied: jinja2 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.6->llama-index) (3.1.6)\n",
      "Requirement already satisfied: openai>=1.1.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.105.0)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (4.13.5)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.7.1)\n",
      "Requirement already satisfied: pandas<2.3.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.2.3)\n",
      "Requirement already satisfied: pypdf<7,>=5.1.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (6.0.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: soupsieve>1.2 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.8)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.14,>=0.13.6->llama-index) (0.4.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (0.10.0)\n",
      "Requirement already satisfied: sniffio in e:\\conda\\envs\\langfuse\\lib\\site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (3.10)\n",
      "Requirement already satisfied: certifi in e:\\conda\\envs\\langfuse\\lib\\site-packages (from httpx->llama-index-core<0.14,>=0.13.6->llama-index) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\conda\\envs\\langfuse\\lib\\site-packages (from httpx->llama-index-core<0.14,>=0.13.6->llama-index) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.14,>=0.13.6->llama-index) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.6->llama-index) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.6->llama-index) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.6->llama-index) (0.4.1)\n",
      "Requirement already satisfied: colorama in e:\\conda\\envs\\langfuse\\lib\\site-packages (from tqdm<5,>=4.66.1->llama-index-core<0.14,>=0.13.6->llama-index) (0.4.6)\n",
      "Requirement already satisfied: mistralai>=1.8.2 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-llms-mistralai) (1.9.6)\n",
      "Requirement already satisfied: llama-cloud==0.1.35 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.35)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.54)\n",
      "Requirement already satisfied: llama-cloud-services>=0.6.54 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.54)\n",
      "Requirement already satisfied: click<9,>=8.1.7 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-cloud-services>=0.6.54->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (8.2.1)\n",
      "Requirement already satisfied: python-dotenv<2,>=1.0.1 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from llama-cloud-services>=0.6.54->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.1.1)\n",
      "Requirement already satisfied: eval-type-backport>=0.2.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from mistralai>=1.8.2->llama-index-llms-mistralai) (0.2.2)\n",
      "Requirement already satisfied: invoke<3.0.0,>=2.2.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from mistralai>=1.8.2->llama-index-llms-mistralai) (2.2.0)\n",
      "Requirement already satisfied: joblib in e:\\conda\\envs\\langfuse\\lib\\site-packages (from nltk>3.8.1->llama-index) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from nltk>3.8.1->llama-index) (2025.9.1)\n",
      "Requirement already satisfied: six>=1.5 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.6->llama-index) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.6->llama-index) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.6->llama-index) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.14,>=0.13.6->llama-index) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from dataclasses-json->llama-index-core<0.14,>=0.13.6->llama-index) (3.26.1)\n",
      "Requirement already satisfied: packaging>=17.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.14,>=0.13.6->llama-index) (24.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.6->llama-index) (3.0.2)\n",
      "Installing collected packages: llama-index-core, llama-index-llms-mistralai, llama-index-embeddings-mistralai, llama-index\n",
      "\n",
      "  Attempting uninstall: llama-index-core\n",
      "\n",
      "    Found existing installation: llama-index-core 0.13.5\n",
      "\n",
      "    Uninstalling llama-index-core-0.13.5:\n",
      "\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "      Successfully uninstalled llama-index-core-0.13.5\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "  Attempting uninstall: llama-index\n",
      "   ---------------------------------------- 0/4 [llama-index-core]\n",
      "   ------------------------------ --------- 3/4 [llama-index]\n",
      "    Found existing installation: llama-index 0.13.5\n",
      "   ------------------------------ --------- 3/4 [llama-index]\n",
      "    Uninstalling llama-index-0.13.5:\n",
      "   ------------------------------ --------- 3/4 [llama-index]\n",
      "      Successfully uninstalled llama-index-0.13.5\n",
      "   ------------------------------ --------- 3/4 [llama-index]\n",
      "   ------------------------------ --------- 3/4 [llama-index]\n",
      "   ---------------------------------------- 4/4 [llama-index]\n",
      "\n",
      "Successfully installed llama-index-0.13.6 llama-index-core-0.13.6 llama-index-embeddings-mistralai-0.4.0 llama-index-llms-mistralai-0.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index llama-index-llms-mistralai llama-index-embeddings-mistralai nest_asyncio --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "468f84ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Mistral API key\n",
    "import os\n",
    " \n",
    "os.environ[\"MISTRAL_API_KEY\"] = \"ChJxeEAnblw7oKELK0cJMVORCMXgrpYn\"\n",
    " \n",
    "# Ensures that sync and async code can be used together without issues\n",
    "import nest_asyncio\n",
    " \n",
    "nest_asyncio.apply()\n",
    " \n",
    "# Import and set up llama index\n",
    "from llama_index.llms.mistralai import MistralAI\n",
    "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
    "from llama_index.core import Settings\n",
    " \n",
    "# Define your LLM and embedding model\n",
    "llm = MistralAI(model=\"open-mixtral-8x22b\", temperature=0.1)\n",
    "embed_model = MistralAIEmbedding(model_name=\"mistral-embed\")\n",
    " \n",
    "# Set the LLM and embedding model in the Settings object\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c14bf3a",
   "metadata": {},
   "source": [
    "#### Step 2: Initialize Langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccb2ca66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: langfuse in e:\\conda\\envs\\langfuse\\lib\\site-packages (3.3.4)\n",
      "Requirement already satisfied: openinference-instrumentation-llama-index in e:\\conda\\envs\\langfuse\\lib\\site-packages (4.3.4)\n",
      "Collecting wget\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\slsxz\\appdata\\roaming\\python\\python312\\site-packages (from langfuse) (2.2.1)\n",
      "Requirement already satisfied: httpx<1.0,>=0.15.4 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from langfuse) (0.28.1)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.33.1 in c:\\users\\slsxz\\appdata\\roaming\\python\\python312\\site-packages (from langfuse) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1 in c:\\users\\slsxz\\appdata\\roaming\\python\\python312\\site-packages (from langfuse) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.33.1 in c:\\users\\slsxz\\appdata\\roaming\\python\\python312\\site-packages (from langfuse) (1.34.1)\n",
      "Requirement already satisfied: packaging<26.0,>=23.2 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from langfuse) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0,>=1.10.7 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from langfuse) (2.11.7)\n",
      "Requirement already satisfied: requests<3,>=2 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from langfuse) (2.32.5)\n",
      "Requirement already satisfied: wrapt<2.0,>=1.14 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from langfuse) (1.17.3)\n",
      "Requirement already satisfied: anyio in e:\\conda\\envs\\langfuse\\lib\\site-packages (from httpx<1.0,>=0.15.4->langfuse) (4.10.0)\n",
      "Requirement already satisfied: certifi in e:\\conda\\envs\\langfuse\\lib\\site-packages (from httpx<1.0,>=0.15.4->langfuse) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\conda\\envs\\langfuse\\lib\\site-packages (from httpx<1.0,>=0.15.4->langfuse) (1.0.9)\n",
      "Requirement already satisfied: idna in e:\\conda\\envs\\langfuse\\lib\\site-packages (from httpx<1.0,>=0.15.4->langfuse) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from httpcore==1.*->httpx<1.0,>=0.15.4->langfuse) (0.16.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\slsxz\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-api<2.0.0,>=1.33.1->langfuse) (8.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from opentelemetry-api<2.0.0,>=1.33.1->langfuse) (4.15.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\slsxz\\appdata\\roaming\\python\\python312\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.33.1->langfuse) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\slsxz\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in c:\\users\\slsxz\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.34.1 in c:\\users\\slsxz\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse) (1.34.1)\n",
      "Requirement already satisfied: protobuf<6.0,>=5.0 in c:\\users\\slsxz\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-proto==1.34.1->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse) (5.29.5)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in c:\\users\\slsxz\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-sdk<2.0.0,>=1.33.1->langfuse) (0.55b1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from pydantic<3.0,>=1.10.7->langfuse) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from pydantic<3.0,>=1.10.7->langfuse) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from pydantic<3.0,>=1.10.7->langfuse) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from requests<3,>=2->langfuse) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from requests<3,>=2->langfuse) (2.5.0)\n",
      "Requirement already satisfied: openinference-instrumentation>=0.1.27 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from openinference-instrumentation-llama-index) (0.1.37)\n",
      "Requirement already satisfied: openinference-semantic-conventions>=0.1.17 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from openinference-instrumentation-llama-index) (0.1.21)\n",
      "Requirement already satisfied: opentelemetry-instrumentation in e:\\conda\\envs\\langfuse\\lib\\site-packages (from openinference-instrumentation-llama-index) (0.55b1)\n",
      "Requirement already satisfied: sniffio>=1.1 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from anyio->httpx<1.0,>=0.15.4->langfuse) (1.3.1)\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py): started\n",
      "  Building wheel for wget (setup.py): finished with status 'done'\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9712 sha256=963763a49bbe535daf69c10fd622f11f9c3b4da9c643419b1e22520dd92c7e1d\n",
      "  Stored in directory: c:\\users\\slsxz\\appdata\\local\\pip\\cache\\wheels\\66\\9e\\4d\\df43e9e196c24d1dcbbc9b362e738a39d63cd4bbfde8745380\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'wget' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'wget'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n"
     ]
    }
   ],
   "source": [
    "%pip install langfuse openinference-instrumentation-llama-index wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a65f60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import os\n",
    " \n",
    "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-b26f6b38-1d21-4efb-b0ed-5057200b2148\" \n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-842c4b9d-4e56-4325-9df5-5a09408f42b1\" \n",
    "\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"http://localhost:3000\"\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"73c80b33ad68446ea3f059efe5c1a65f.T2PZjYiHcT2JYx2a\"\n",
    "# os.environ[\"OPENAI_BASE_URL\"] = \"https://open.bigmodel.cn/api/paas/v4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8778d72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langfuse client is authenticated and ready!\n"
     ]
    }
   ],
   "source": [
    "from langfuse import get_client\n",
    " \n",
    "langfuse = get_client()\n",
    " \n",
    "# Verify connection\n",
    "if langfuse.auth_check():\n",
    "    print(\"Langfuse client is authenticated and ready!\")\n",
    "else:\n",
    "    print(\"Authentication failed. Please check your credentials and host.\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ad440",
   "metadata": {},
   "source": [
    "Initialize the [OpenInference LlamaIndex instrumentation](https://arize.com/docs/phoenix/integrations/frameworks/llamaindex). This third-party instrumentation **automatically captures LlamaIndex operations and exports OpenTelemetry (OTel) spans to Langfuse.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f395f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
    " \n",
    "# Initialize LlamaIndex instrumentation\n",
    "LlamaIndexInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c845329a",
   "metadata": {},
   "source": [
    "#### Step 3: Download data\n",
    "\n",
    "We download the file we want to use for RAG. In this example, we use a hedgehog care guide pdf file to enable the language model to answer questions about caring for hedgehogs ðŸ¦”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc8aa956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    " \n",
    "# url = \"https://www.pro-igel.de/downloads/merkblaetter_engl/wildtier_engl.pdf\"\n",
    "# url = \"https://earps.org/media/2022/02/Hedgehog-Care-Guide.pdf\"\n",
    "# wget.download(url, \"./hedgehog.pdf\")   # saves as ./hedgehog.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28844d7",
   "metadata": {},
   "source": [
    "Load the pdf using the LlamaIndex [SimpleDirectoryReader](https://docs.llamaindex.ai/en/stable/module_guides/loading/simpledirectoryreader/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "399558c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    " \n",
    "hedgehog_docs = SimpleDirectoryReader(\n",
    "    input_files=[\"./hedgehog.pdf\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed30df5a",
   "metadata": {},
   "source": [
    "#### Step 4: Build RAG on the hedgehog doc\n",
    "\n",
    "create vector embeddings of the hedgehog document using [VectorStoreIndex](https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index/) and then convert it into a [queryable engine](https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/) to retrieve information based on queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25339f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    " \n",
    "hedgehog_index = VectorStoreIndex.from_documents(hedgehog_docs)\n",
    "hedgehog_query_engine = hedgehog_index.as_query_engine(similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304dd9ec",
   "metadata": {},
   "source": [
    "Finally, to put everything together, we query the engine and print a response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f37310a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hedgehogs that are affected by Wobbly Hedgehog Syndrome (WHS) require help. As the condition progresses, they may need assistance with feeding and hydration through syringe feeding, as well as support for mobility and independence. Additionally, hedgehogs that have been injured or are experiencing other health issues may require help, including immediate veterinary treatment.\n"
     ]
    }
   ],
   "source": [
    "response = hedgehog_query_engine.query(\"Which hedgehogs require help?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef99ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "      <th>total_tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>open-mixtral-8x22b</th>\n",
       "      <td>3263</td>\n",
       "      <td>98</td>\n",
       "      <td>3361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    input_tokens  output_tokens  total_tokens\n",
       "model                                                        \n",
       "open-mixtral-8x22b          3263             98          3361"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarize cost by model\n",
    "import pandas as pd\n",
    "\n",
    "trace = langfuse.api.trace.get(\"e5cf526e8c6b302c496cf9305e697916\")\n",
    "observations = trace.observations\n",
    "\n",
    "\n",
    "def summarize_usage(observations):\n",
    "    \"\"\"Summarize usage data grouped by model.\"\"\"\n",
    "    usage_data = []\n",
    "\n",
    "    for obs in observations:\n",
    "        usage = obs.usage\n",
    "        if usage:\n",
    "            usage_data.append(\n",
    "                {\n",
    "                    \"model\": obs.model,\n",
    "                    \"input_tokens\": usage.input,\n",
    "                    \"output_tokens\": usage.output,\n",
    "                    \"total_tokens\": usage.total,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    df = pd.DataFrame(usage_data)\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    summary = df.groupby(\"model\").sum()\n",
    "    return summary\n",
    "\n",
    "\n",
    "# Example usage (assuming `observations` is defined as in the provided code):\n",
    "summary_df = summarize_usage(observations)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe61bf8",
   "metadata": {},
   "source": [
    "#### Step 5: (Optional) Implement user feedback to see how your application is performing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d19cf5",
   "metadata": {},
   "source": [
    "To monitor the quality of your hedgehog chat application, you can use Langfuse Scores to store user feedback (e.g. thumps up/down or comments). These scores can then be analyzed in PostHog.\n",
    "\n",
    "Scores are used to evaluate single observations or entire traces. You can create them via the annotation workflow in the Langfuse UI, run model-based evaluation or ingest via the SDK as we do it in this example.\n",
    "\n",
    "To get the context of the current observation, we use the observe() decorator and apply it to the hedgehog_helper() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1548db75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before deciding to keep a hedgehog as a pet, it's important to consider several factors. Firstly, ensure you have enough time and attention for a hedgehog and a suitable place to keep its cage away from other pets. Remember, hedgehogs are nocturnal animals and their schedule cannot be changed.\n",
      "\n",
      "You should also be prepared to handle the hedgehog's quills, which are sharper than they appear. It's advisable to visit a reputable breeder or someone who owns hedgehogs to see what the quills are like.\n",
      "\n",
      "Hedgehogs require a constant temperature of 73-78Â°F (23-25Â°C) in their cage to prevent them from attempting hibernation, which can be dangerous. This means you'll need a space heater, ceramic heat emitter, or other type of heating setup.\n",
      "\n",
      "It's also crucial to understand that hedgehogs are solitary animals and should not be housed together. Males should never be housed together as they are territorial and almost never get along. Female-female pairing can work under the right circumstances, but introductions should be done carefully.\n",
      "\n",
      "Lastly, consider the expenses involved in keeping a hedgehog. They can be quite expensive to keep, and vet bills can quickly add up. You should always have money saved for vet bills and be prepared to spend it. The initial cost of getting a hedgehog can be quite high as well, with the supplies adding up to around $450 and $150-$250 for the hedgehog.\n",
      "\n",
      "If you can meet all these requirements and understand the responsibilities involved, a hedgehog might be a great fit for you. However, if you live in Arizona, California, Georgia, Hawaii, Pennsylvania, Fairfax County, VA, the 5 Boroughs of NY, or Washington, DC, owning a hedgehog is prohibited.\n"
     ]
    }
   ],
   "source": [
    "from langfuse import observe, get_client\n",
    "\n",
    "langfuse = get_client()\n",
    "\n",
    "# Langfuse observe() decorator to automatically create a trace for the top-level function and spans for any nested functions.\n",
    "@observe()\n",
    "def hedgehog_helper(user_message):\n",
    "    response = hedgehog_query_engine.query(user_message)\n",
    "    trace_id = langfuse.get_current_trace_id()\n",
    " \n",
    "    print(response)\n",
    " \n",
    "    return trace_id\n",
    "\n",
    "trace_id = hedgehog_helper(\"Can I keep the hedgehog as a pet?\")\n",
    "\n",
    "# Score the trace\n",
    "langfuse.create_score(\n",
    "    trace_id=trace_id,\n",
    "    name=\"user-explicit-feedback\",\n",
    "    value=0.9,\n",
    "    data_type=\"NUMERIC\",  # optional, inferred if not provided\n",
    "    comment=\"Good to know!\",  # optional\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cc45f4",
   "metadata": {},
   "source": [
    "#### Step 6: See your data in PostHog\n",
    "\n",
    "[PostHog](https://us.posthog.com/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
