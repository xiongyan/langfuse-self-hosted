{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fc2990f",
   "metadata": {},
   "source": [
    "## [Observability for OpenAI Assistants API with Langfuse ](https://langfuse.com/guides/cookbook/integration_openai_assistants)\n",
    "\n",
    "\n",
    "how to use the Langfuse observe decorator to trace calls made to the [OpenAI Assistants API](https://platform.openai.com/docs/assistants/migration). It covers creating an assistant, running it on a thread, and observing the execution with Langfuse tracing.\n",
    "\n",
    "Note: The native OpenAI SDK wrapper does not support tracing of the OpenAI assistants API, you need to instrument it via the decorator as shown in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99838c2f",
   "metadata": {},
   "source": [
    "### Step 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e676861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: openai in e:\\conda\\envs\\langfuse\\lib\\site-packages (1.107.1)\n",
      "Requirement already satisfied: langfuse in e:\\conda\\envs\\langfuse\\lib\\site-packages (2.60.9)\n",
      "Collecting langfuse\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/dc/46/edd370d47ca72ed4ca36b3c3b8f3a4ce71a310629d0bcb6ed760b04b08b1/langfuse-3.3.4-py3-none-any.whl (318 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in e:\\conda\\envs\\langfuse\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in e:\\conda\\envs\\langfuse\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\conda\\envs\\langfuse\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\slsxz\\appdata\\roaming\\python\\python312\\site-packages (from langfuse) (2.2.1)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.33.1 in c:\\users\\slsxz\\appdata\\roaming\\python\\python312\\site-packages (from langfuse) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1 in c:\\users\\slsxz\\appdata\\roaming\\python\\python312\\site-packages (from langfuse) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.33.1 in c:\\users\\slsxz\\appdata\\roaming\\python\\python312\\site-packages (from langfuse) (1.34.1)\n",
      "Requirement already satisfied: packaging<26.0,>=23.2 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from langfuse) (24.2)\n",
      "Requirement already satisfied: requests<3,>=2 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from langfuse) (2.32.5)\n",
      "Requirement already satisfied: wrapt<2.0,>=1.14 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from langfuse) (1.17.3)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\slsxz\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-api<2.0.0,>=1.33.1->langfuse) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\slsxz\\appdata\\roaming\\python\\python312\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.33.1->langfuse) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\slsxz\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in c:\\users\\slsxz\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.34.1 in c:\\users\\slsxz\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse) (1.34.1)\n",
      "Requirement already satisfied: protobuf<6.0,>=5.0 in c:\\users\\slsxz\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-proto==1.34.1->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse) (5.29.5)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in c:\\users\\slsxz\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-sdk<2.0.0,>=1.33.1->langfuse) (0.55b1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from requests<3,>=2->langfuse) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from requests<3,>=2->langfuse) (2.5.0)\n",
      "Requirement already satisfied: colorama in e:\\conda\\envs\\langfuse\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Installing collected packages: langfuse\n",
      "  Attempting uninstall: langfuse\n",
      "    Found existing installation: langfuse 2.60.9\n",
      "    Uninstalling langfuse-2.60.9:\n",
      "      Successfully uninstalled langfuse-2.60.9\n",
      "Successfully installed langfuse-3.3.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langfuse-haystack 2.2.0 requires langfuse<3.0.0,>=2.9.0, but you have langfuse 3.3.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade openai langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "773c319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-962190cc-b2bd-47c0-b752-8de287a2a5c1\"\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-4e9959c3-0935-4142-b789-734beb81d15a\"\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"http://localhost:3000\"\n",
    "\n",
    "# Your openai key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"f0c1fb9f5c534e55a66d9e539916fdb0.GQKa6HaX6MpT9ioJ\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"73c80b33ad68446ea3f059efe5c1a65f.T2PZjYiHcT2JYx2a\"\n",
    "os.environ[\"OPENAI_BASE_URL\"] = \"https://open.bigmodel.cn/api/paas/v4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af174130",
   "metadata": {},
   "source": [
    "### Step1: Create an Assistant\n",
    "\n",
    "Use the client.beta.assistants.create method to create a new assistant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93adc0f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'timestamp': '2025-09-12T06:05:13.751+00:00', 'status': 404, 'error': 'Not Found', 'path': '/v4/assistants'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      7\u001b[39m     assistant = client.beta.assistants.create(\n\u001b[32m      8\u001b[39m         name=\u001b[33m\"\u001b[39m\u001b[33mMath Tutor\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m         instructions=\u001b[33m\"\u001b[39m\u001b[33mYou are a personal math tutor. Answer questions briefly, in a sentence or less.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m         model=\u001b[33m\"\u001b[39m\u001b[33mglm-4.5-flash\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m     )\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m assistant\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m assistant = \u001b[43mcreate_assistant\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreated assistant: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00massistant.id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\conda\\envs\\langfuse\\Lib\\site-packages\\langfuse\\_client\\observe.py:426\u001b[39m, in \u001b[36mLangfuseDecorator._sync_observe.<locals>.sync_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    422\u001b[39m     langfuse_span_or_generation.update(\n\u001b[32m    423\u001b[39m         level=\u001b[33m\"\u001b[39m\u001b[33mERROR\u001b[39m\u001b[33m\"\u001b[39m, status_message=\u001b[38;5;28mstr\u001b[39m(e)\n\u001b[32m    424\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    428\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_return_type_generator:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\conda\\envs\\langfuse\\Lib\\site-packages\\langfuse\\_client\\observe.py:397\u001b[39m, in \u001b[36mLangfuseDecorator._sync_observe.<locals>.sync_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    394\u001b[39m is_return_type_generator = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m397\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m capture_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    400\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isgenerator(result):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mcreate_assistant\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;129m@observe\u001b[39m()\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_assistant\u001b[39m():\n\u001b[32m      6\u001b[39m     client = OpenAI()\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     assistant = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m.\u001b[49m\u001b[43massistants\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMath Tutor\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mYou are a personal math tutor. Answer questions briefly, in a sentence or less.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mglm-4.5-flash\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m assistant\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\conda\\envs\\langfuse\\Lib\\site-packages\\openai\\resources\\beta\\assistants.py:154\u001b[39m, in \u001b[36mAssistants.create\u001b[39m\u001b[34m(self, model, description, instructions, metadata, name, reasoning_effort, response_format, temperature, tool_resources, tools, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[33;03mCreate an assistant with a model and instructions.\u001b[39;00m\n\u001b[32m     77\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    151\u001b[39m \u001b[33;03m  timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    153\u001b[39m extra_headers = {\u001b[33m\"\u001b[39m\u001b[33mOpenAI-Beta\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33massistants=v2\u001b[39m\u001b[33m\"\u001b[39m, **(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/assistants\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdescription\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minstructions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mname\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_resources\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_resources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43massistant_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAssistantCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mAssistant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\conda\\envs\\langfuse\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\conda\\envs\\langfuse\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNotFoundError\u001b[39m: Error code: 404 - {'timestamp': '2025-09-12T06:05:13.751+00:00', 'status': 404, 'error': 'Not Found', 'path': '/v4/assistants'}"
     ]
    }
   ],
   "source": [
    "# --Fail\n",
    "from langfuse import observe\n",
    "from openai import OpenAI\n",
    "\n",
    "@observe()\n",
    "def create_assistant():\n",
    "    client = OpenAI()\n",
    "    assistant = client.beta.assistants.create(\n",
    "        name=\"Math Tutor\",\n",
    "        instructions=\"You are a personal math tutor. Answer questions briefly, in a sentence or less.\",\n",
    "        model=\"glm-4.5-flash\"\n",
    "    )\n",
    "    return assistant\n",
    "\n",
    "assistant = create_assistant()\n",
    "print(f\"Created assistant: {assistant.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd6f265",
   "metadata": {},
   "source": [
    "### Step 2: Running the Assistant\n",
    "\n",
    "Create a thread and run the assistant on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e57968f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'assistant' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m run, thread\n\u001b[32m     24\u001b[39m user_input = \u001b[33m\"\u001b[39m\u001b[33mI need to solve the equation `3x + 11 = 14`. Can you help me?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m run, thread = run_assistant(\u001b[43massistant\u001b[49m.id, user_input)\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreated run: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun.id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'assistant' is not defined"
     ]
    }
   ],
   "source": [
    "@observe()\n",
    "def run_assistant(assistant_id, user_input):\n",
    "    client = OpenAI()\n",
    "\n",
    "    thread = client.beta.threads.create()\n",
    "\n",
    "    client.beta.threads.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        role=\"assistant\",\n",
    "        content=\"I am a math tutor that likes to help math students, how can I help?\",\n",
    "    )\n",
    "\n",
    "    client.beta.threads.messages.create(\n",
    "        thread_id=thread.id, role=\"user\", content=user_input\n",
    "    )\n",
    "\n",
    "    run = client.beta.threads.runs.create(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=assistant_id,\n",
    "    )\n",
    "\n",
    "    return run, thread\n",
    "\n",
    "user_input = \"I need to solve the equation `3x + 11 = 14`. Can you help me?\"\n",
    "run, thread = run_assistant(assistant.id, user_input)\n",
    "print(f\"Created run: {run.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45be8b54",
   "metadata": {},
   "source": [
    "### Step 2: Getting the Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ac8cd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langfuse import get_client\n",
    "langfuse = get_client()\n",
    "\n",
    "@observe()\n",
    "def get_response(thread_id, run_id):\n",
    "    client = OpenAI()\n",
    "\n",
    "    messages = client.beta.threads.messages.list(thread_id=thread_id, order=\"asc\")\n",
    "    assistant_response = messages.data[-1].content[0].text.value\n",
    "    # get run for token counts\n",
    "    run_log = client.beta.threads.runs.retrieve(\n",
    "        thread_id=thread_id,\n",
    "        run_id=run_id\n",
    "    )\n",
    " \n",
    "    message_log = client.beta.threads.messages.list(\n",
    "        thread_id=thread_id,\n",
    "    )\n",
    "\n",
    "    input_messages = [{\"role\": message.role, \"content\": message.content[0].text.value} for message in message_log.data[::-1][:-1]]\n",
    "\n",
    "    langfuse_client = langfuse.client_instance\n",
    "    # pass trace_id and current observation ids to the newly created child generation\n",
    "    langfuse_client.generation(\n",
    "        trace_id=langfuse.get_current_trace_id(),\n",
    "        parent_observation_id=langfuse.get_current_observation_id(),\n",
    "        model=run.model,\n",
    "        usage_details=run.usage,\n",
    "        input=input_messages,\n",
    "        output=assistant_response\n",
    "    )\n",
    "\n",
    "    return assistant_response, run\n",
    "\n",
    "response = get_response(thread.id, run.id)\n",
    "print(f\"Assistant response: {response[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6395080b",
   "metadata": {},
   "source": [
    "### All in one trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da777eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    " \n",
    "@observe()\n",
    "def run_math_tutor(user_input):\n",
    "    assistant = create_assistant()\n",
    "    run, thread = run_assistant(assistant.id, user_input)\n",
    " \n",
    "    time.sleep(5) # notebook only, wait for the assistant to finish\n",
    " \n",
    "    response = get_response(thread.id, run.id)\n",
    "    \n",
    "    return response[0]\n",
    " \n",
    "user_input = \"I need to solve the equation `3x + 11 = 14`. Can you help me?\"\n",
    "response = run_math_tutor(user_input)\n",
    "print(f\"Assistant response: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
