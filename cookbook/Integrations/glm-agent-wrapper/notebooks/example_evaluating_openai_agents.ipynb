{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Example: Evaluating GLM Model with OpenAI Agents\n",
                "\n",
                "This notebook demonstrates how to evaluate the GLM Model integrated with the OpenAI Agents framework. We will implement a model wrapper that adheres to the `agents.models.interface.Model` interface, specifically the `get_response` and `stream_response` methods."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import base64\n",
                "\n",
                " \n",
                "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
                "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-962190cc-b2bd-47c0-b752-8de287a2a5c1\" \n",
                "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-4e9959c3-0935-4142-b789-734beb81d15a\" \n",
                "os.environ[\"LANGFUSE_HOST\"] = \"http://localhost:3000\"\n",
                " \n",
                "# Build Basic Auth header.\n",
                "LANGFUSE_AUTH = base64.b64encode(\n",
                "    f\"{os.environ.get('LANGFUSE_PUBLIC_KEY')}:{os.environ.get('LANGFUSE_SECRET_KEY')}\".encode()\n",
                ").decode()\n",
                " \n",
                "# Configure OpenTelemetry endpoint & headers\n",
                "os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = os.environ.get(\"LANGFUSE_HOST\") + \"/api/public/otel\"\n",
                "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"Authorization=Basic {LANGFUSE_AUTH}\"\n",
                " \n",
                "# Your openai key\n",
                "# os.environ[\"OPEN_AI_URL\"] = \"https://open.bigmodel.cn/api/paas/v4/\"\n",
                "os.environ[\"OPENAI_API_KEY\"] = \"f0c1fb9f5c534e55a66d9e539916fdb0.GQKa6HaX6MpT9ioJ\"\n",
                "os.environ[\"OPENAI_BASE_URL\"] = \"https://open.bigmodel.cn/api/paas/v4\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langfuse import get_client\n",
                " \n",
                "langfuse = get_client()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "import nest_asyncio\n",
                "nest_asyncio.apply()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Overriding of current TracerProvider is not allowed\n"
                    ]
                }
            ],
            "source": [
                "import logfire\n",
                "\n",
                "# Configure logfire instrumentation\n",
                "logfire.configure(\n",
                "    service_name=\"my_agent_service\",\n",
                "    send_to_logfire=False,\n",
                ")\n",
                "\n",
                "# The method automatically patches the OpenAI Agents SDK to send logs via OTLP to Langfuse.\n",
                "logfire.instrument_openai_agents()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "16:03:23.696 OpenAI Agents trace: Agent workflow\n",
                        "16:03:23.697   Agent run: 'GLM Assistant'\n",
                        "16:03:23.700     Responses API with 'gpt-4o'\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Error getting response: Error code: 404 - {'timestamp': '2025-08-20T08:03:25.869+00:00', 'status': 404, 'error': 'Not Found', 'path': '/v4/responses'}. (request_id: None)\n"
                    ]
                },
                {
                    "ename": "NotFoundError",
                    "evalue": "Error code: 404 - {'timestamp': '2025-08-20T08:03:25.869+00:00', 'status': 404, 'error': 'Not Found', 'path': '/v4/responses'}",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m     \u001b[38;5;28mprint\u001b[39m(result.final_output)\n\u001b[32m     25\u001b[39m loop = asyncio.get_event_loop()\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m langfuse.flush()\n",
                        "\u001b[36mFile \u001b[39m\u001b[32me:\\conda\\envs\\langfuse\\Lib\\site-packages\\nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32me:\\conda\\envs\\langfuse\\Lib\\asyncio\\futures.py:202\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
                        "\u001b[36mFile \u001b[39m\u001b[32me:\\conda\\envs\\langfuse\\Lib\\asyncio\\tasks.py:316\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    314\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._must_cancel:\n\u001b[32m    319\u001b[39m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m     14\u001b[39m     agent = CustomAgent(\n\u001b[32m     15\u001b[39m         name=\u001b[33m\"\u001b[39m\u001b[33mGLM Assistant\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m         instructions=\u001b[33m\"\u001b[39m\u001b[33mYou are a helpful assistant using the GLM model.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     17\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m Runner.run(\n\u001b[32m     19\u001b[39m         agent,\n\u001b[32m     20\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExplain the importance of evaluating AI agents.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     21\u001b[39m         run_config=RunConfig(model=glm_model)\n\u001b[32m     22\u001b[39m     )\n\u001b[32m     23\u001b[39m     \u001b[38;5;28mprint\u001b[39m(result.final_output)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32me:\\conda\\envs\\langfuse\\Lib\\site-packages\\agents\\run.py:237\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id, session)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run a workflow starting at the given agent. The agent will run in a loop until a final\u001b[39;00m\n\u001b[32m    211\u001b[39m \u001b[33;03moutput is generated. The loop runs like so:\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[33;03m1. The agent is invoked with the given input.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    234\u001b[39m \u001b[33;03m    agent. Agents may perform handoffs, so we don't know the specific type of the output.\u001b[39;00m\n\u001b[32m    235\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    236\u001b[39m runner = DEFAULT_AGENT_RUNNER\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m runner.run(\n\u001b[32m    238\u001b[39m     starting_agent,\n\u001b[32m    239\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    240\u001b[39m     context=context,\n\u001b[32m    241\u001b[39m     max_turns=max_turns,\n\u001b[32m    242\u001b[39m     hooks=hooks,\n\u001b[32m    243\u001b[39m     run_config=run_config,\n\u001b[32m    244\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m    245\u001b[39m     session=session,\n\u001b[32m    246\u001b[39m )\n",
                        "\u001b[36mFile \u001b[39m\u001b[32me:\\conda\\envs\\langfuse\\Lib\\site-packages\\agents\\run.py:443\u001b[39m, in \u001b[36mAgentRunner.run\u001b[39m\u001b[34m(self, starting_agent, input, **kwargs)\u001b[39m\n\u001b[32m    438\u001b[39m logger.debug(\n\u001b[32m    439\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_agent.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (turn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_turn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    440\u001b[39m )\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_turn == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m     input_guardrail_results, turn_result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    444\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_input_guardrails(\n\u001b[32m    445\u001b[39m             starting_agent,\n\u001b[32m    446\u001b[39m             starting_agent.input_guardrails\n\u001b[32m    447\u001b[39m             + (run_config.input_guardrails \u001b[38;5;129;01mor\u001b[39;00m []),\n\u001b[32m    448\u001b[39m             _copy_str_or_list(prepared_input),\n\u001b[32m    449\u001b[39m             context_wrapper,\n\u001b[32m    450\u001b[39m         ),\n\u001b[32m    451\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_single_turn(\n\u001b[32m    452\u001b[39m             agent=current_agent,\n\u001b[32m    453\u001b[39m             all_tools=all_tools,\n\u001b[32m    454\u001b[39m             original_input=original_input,\n\u001b[32m    455\u001b[39m             generated_items=generated_items,\n\u001b[32m    456\u001b[39m             hooks=hooks,\n\u001b[32m    457\u001b[39m             context_wrapper=context_wrapper,\n\u001b[32m    458\u001b[39m             run_config=run_config,\n\u001b[32m    459\u001b[39m             should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001b[32m    460\u001b[39m             tool_use_tracker=tool_use_tracker,\n\u001b[32m    461\u001b[39m             previous_response_id=previous_response_id,\n\u001b[32m    462\u001b[39m         ),\n\u001b[32m    463\u001b[39m     )\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    465\u001b[39m     turn_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_single_turn(\n\u001b[32m    466\u001b[39m         agent=current_agent,\n\u001b[32m    467\u001b[39m         all_tools=all_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m    475\u001b[39m         previous_response_id=previous_response_id,\n\u001b[32m    476\u001b[39m     )\n",
                        "\u001b[36mFile \u001b[39m\u001b[32me:\\conda\\envs\\langfuse\\Lib\\asyncio\\tasks.py:385\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    388\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32me:\\conda\\envs\\langfuse\\Lib\\asyncio\\tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32me:\\conda\\envs\\langfuse\\Lib\\site-packages\\agents\\run.py:1036\u001b[39m, in \u001b[36mAgentRunner._run_single_turn\u001b[39m\u001b[34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, previous_response_id)\u001b[39m\n\u001b[32m   1033\u001b[39m \u001b[38;5;28minput\u001b[39m = ItemHelpers.input_to_new_input_list(original_input)\n\u001b[32m   1034\u001b[39m \u001b[38;5;28minput\u001b[39m.extend([generated_item.to_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_new_response(\n\u001b[32m   1037\u001b[39m     agent,\n\u001b[32m   1038\u001b[39m     system_prompt,\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1040\u001b[39m     output_schema,\n\u001b[32m   1041\u001b[39m     all_tools,\n\u001b[32m   1042\u001b[39m     handoffs,\n\u001b[32m   1043\u001b[39m     context_wrapper,\n\u001b[32m   1044\u001b[39m     run_config,\n\u001b[32m   1045\u001b[39m     tool_use_tracker,\n\u001b[32m   1046\u001b[39m     previous_response_id,\n\u001b[32m   1047\u001b[39m     prompt_config,\n\u001b[32m   1048\u001b[39m )\n\u001b[32m   1050\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_single_step_result_from_response(\n\u001b[32m   1051\u001b[39m     agent=agent,\n\u001b[32m   1052\u001b[39m     original_input=original_input,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1061\u001b[39m     tool_use_tracker=tool_use_tracker,\n\u001b[32m   1062\u001b[39m )\n",
                        "\u001b[36mFile \u001b[39m\u001b[32me:\\conda\\envs\\langfuse\\Lib\\site-packages\\agents\\run.py:1256\u001b[39m, in \u001b[36mAgentRunner._get_new_response\u001b[39m\u001b[34m(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, context_wrapper, run_config, tool_use_tracker, previous_response_id, prompt_config)\u001b[39m\n\u001b[32m   1253\u001b[39m model_settings = agent.model_settings.resolve(run_config.model_settings)\n\u001b[32m   1254\u001b[39m model_settings = RunImpl.maybe_reset_tool_choice(agent, tool_use_tracker, model_settings)\n\u001b[32m-> \u001b[39m\u001b[32m1256\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m model.get_response(\n\u001b[32m   1257\u001b[39m     system_instructions=filtered.instructions,\n\u001b[32m   1258\u001b[39m     \u001b[38;5;28minput\u001b[39m=filtered.input,\n\u001b[32m   1259\u001b[39m     model_settings=model_settings,\n\u001b[32m   1260\u001b[39m     tools=all_tools,\n\u001b[32m   1261\u001b[39m     output_schema=output_schema,\n\u001b[32m   1262\u001b[39m     handoffs=handoffs,\n\u001b[32m   1263\u001b[39m     tracing=get_model_tracing_impl(\n\u001b[32m   1264\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m   1265\u001b[39m     ),\n\u001b[32m   1266\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m   1267\u001b[39m     prompt=prompt_config,\n\u001b[32m   1268\u001b[39m )\n\u001b[32m   1270\u001b[39m context_wrapper.usage.add(new_response.usage)\n\u001b[32m   1272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_response\n",
                        "\u001b[36mFile \u001b[39m\u001b[32me:\\conda\\envs\\langfuse\\Lib\\site-packages\\agents\\models\\openai_responses.py:83\u001b[39m, in \u001b[36mOpenAIResponsesModel.get_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id, prompt)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m response_span(disabled=tracing.is_disabled()) \u001b[38;5;28;01mas\u001b[39;00m span_response:\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m     84\u001b[39m             system_instructions,\n\u001b[32m     85\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m     86\u001b[39m             model_settings,\n\u001b[32m     87\u001b[39m             tools,\n\u001b[32m     88\u001b[39m             output_schema,\n\u001b[32m     89\u001b[39m             handoffs,\n\u001b[32m     90\u001b[39m             previous_response_id,\n\u001b[32m     91\u001b[39m             stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     92\u001b[39m             prompt=prompt,\n\u001b[32m     93\u001b[39m         )\n\u001b[32m     95\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _debug.DONT_LOG_MODEL_DATA:\n\u001b[32m     96\u001b[39m             logger.debug(\u001b[33m\"\u001b[39m\u001b[33mLLM responded\u001b[39m\u001b[33m\"\u001b[39m)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32me:\\conda\\envs\\langfuse\\Lib\\site-packages\\agents\\models\\openai_responses.py:279\u001b[39m, in \u001b[36mOpenAIResponsesModel._fetch_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, previous_response_id, stream, prompt)\u001b[39m\n\u001b[32m    276\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    277\u001b[39m         response_format = {\u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: model_settings.verbosity}\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.responses.create(\n\u001b[32m    280\u001b[39m     previous_response_id=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(previous_response_id),\n\u001b[32m    281\u001b[39m     instructions=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(system_instructions),\n\u001b[32m    282\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    283\u001b[39m     \u001b[38;5;28minput\u001b[39m=list_input,\n\u001b[32m    284\u001b[39m     include=include,\n\u001b[32m    285\u001b[39m     tools=converted_tools.tools,\n\u001b[32m    286\u001b[39m     prompt=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(prompt),\n\u001b[32m    287\u001b[39m     temperature=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.temperature),\n\u001b[32m    288\u001b[39m     top_p=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.top_p),\n\u001b[32m    289\u001b[39m     truncation=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.truncation),\n\u001b[32m    290\u001b[39m     max_output_tokens=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.max_tokens),\n\u001b[32m    291\u001b[39m     tool_choice=tool_choice,\n\u001b[32m    292\u001b[39m     parallel_tool_calls=parallel_tool_calls,\n\u001b[32m    293\u001b[39m     stream=stream,\n\u001b[32m    294\u001b[39m     extra_headers={**_HEADERS, **(model_settings.extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})},\n\u001b[32m    295\u001b[39m     extra_query=model_settings.extra_query,\n\u001b[32m    296\u001b[39m     extra_body=model_settings.extra_body,\n\u001b[32m    297\u001b[39m     text=response_format,\n\u001b[32m    298\u001b[39m     store=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.store),\n\u001b[32m    299\u001b[39m     reasoning=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.reasoning),\n\u001b[32m    300\u001b[39m     metadata=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.metadata),\n\u001b[32m    301\u001b[39m     **extra_args,\n\u001b[32m    302\u001b[39m )\n",
                        "\u001b[36mFile \u001b[39m\u001b[32me:\\conda\\envs\\langfuse\\Lib\\site-packages\\openai\\resources\\responses\\responses.py:2118\u001b[39m, in \u001b[36mAsyncResponses.create\u001b[39m\u001b[34m(self, background, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2082\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2083\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2084\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2116\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   2117\u001b[39m ) -> Response | AsyncStream[ResponseStreamEvent]:\n\u001b[32m-> \u001b[39m\u001b[32m2118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2119\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/responses\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2120\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2121\u001b[39m             {\n\u001b[32m   2122\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mbackground\u001b[39m\u001b[33m\"\u001b[39m: background,\n\u001b[32m   2123\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minclude\u001b[39m\u001b[33m\"\u001b[39m: include,\n\u001b[32m   2124\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   2125\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minstructions\u001b[39m\u001b[33m\"\u001b[39m: instructions,\n\u001b[32m   2126\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_output_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_output_tokens,\n\u001b[32m   2127\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: max_tool_calls,\n\u001b[32m   2128\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2129\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2130\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2131\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprevious_response_id\u001b[39m\u001b[33m\"\u001b[39m: previous_response_id,\n\u001b[32m   2132\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: prompt,\n\u001b[32m   2133\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2134\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m: reasoning,\n\u001b[32m   2135\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2136\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2137\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2138\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2139\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2140\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2141\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: text,\n\u001b[32m   2142\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2143\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2144\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2145\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2146\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtruncation\u001b[39m\u001b[33m\"\u001b[39m: truncation,\n\u001b[32m   2147\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2148\u001b[39m             },\n\u001b[32m   2149\u001b[39m             response_create_params.ResponseCreateParamsStreaming\n\u001b[32m   2150\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2151\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m response_create_params.ResponseCreateParamsNonStreaming,\n\u001b[32m   2152\u001b[39m         ),\n\u001b[32m   2153\u001b[39m         options=make_request_options(\n\u001b[32m   2154\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2155\u001b[39m         ),\n\u001b[32m   2156\u001b[39m         cast_to=Response,\n\u001b[32m   2157\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2158\u001b[39m         stream_cls=AsyncStream[ResponseStreamEvent],\n\u001b[32m   2159\u001b[39m     )\n",
                        "\u001b[36mFile \u001b[39m\u001b[32me:\\conda\\envs\\langfuse\\Lib\\site-packages\\openai\\_base_client.py:1794\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1780\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1781\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1782\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1789\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1790\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1791\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1792\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1793\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32me:\\conda\\envs\\langfuse\\Lib\\site-packages\\openai\\_base_client.py:1594\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1591\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1593\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1594\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1596\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1598\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
                        "\u001b[31mNotFoundError\u001b[39m: Error code: 404 - {'timestamp': '2025-08-20T08:03:25.869+00:00', 'status': 404, 'error': 'Not Found', 'path': '/v4/responses'}"
                    ]
                }
            ],
            "source": [
                "import asyncio\n",
                "from agents import Agent, Runner, RunConfig\n",
                "from glm_agent_wrapper.models.glm_model import GLMModel\n",
                "\n",
                "# Initialize the GLM Model\n",
                "glm_model = GLMModel(model_name=\"GLM-4.5\")\n",
                "\n",
                "class CustomAgent(Agent):\n",
                "    async def run(self, task: str):\n",
                "        response = await glm_model.get_response(task)\n",
                "        return response\n",
                "\n",
                "async def main():\n",
                "    agent = CustomAgent(\n",
                "        name=\"GLM Assistant\",\n",
                "        instructions=\"You are a helpful assistant using the GLM model.\"\n",
                "    )\n",
                "    result = await Runner.run(\n",
                "        agent,\n",
                "        \"Explain the importance of evaluating AI agents.\",\n",
                "        run_config=RunConfig(model=glm_model)\n",
                "    )\n",
                "    print(result.final_output)\n",
                "\n",
                "loop = asyncio.get_event_loop()\n",
                "loop.run_until_complete(main())\n",
                "\n",
                "langfuse.flush()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'The capital of France is **Paris**.  \\n\\nHere are a few key details:  \\n- **Location**: Situated in the north-central part of France, along the Seine River.  \\n- **Significance**: Paris is not only the political capital but also the country’s largest city, a global hub for art, fashion, gastronomy, and culture.  \\n- **Landmarks**: Home to iconic sites like the Eiffel Tower, Louvre Museum, Notre-Dame Cathedral, and the Champs-Élysées.  \\n\\nParis has been the capital of France since at least the 10th century and remains the heart of French governance, economy, and cultural identity.'"
                        ]
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "glm_model = GLMModel(model_name=\"GLM-4.5\")\n",
                "\n",
                "glm_model.get_response(\"What is the capital of France?\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Summary\n",
                "In this notebook, we have set up a custom agent that utilizes the GLM model for generating responses. The `GLMModel` class implements the required methods from the `agents.models.interface.Model` interface, allowing for seamless integration with the agent framework."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "langfuse",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
