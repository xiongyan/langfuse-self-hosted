{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3137bc90",
   "metadata": {},
   "source": [
    "### Haystack Integration\n",
    "\n",
    "[Haystack](https://haystack.deepset.ai/) is the open-source Python framework developed by deepset. Its modular design allows users to implement custom pipelines to build production-ready LLM applications, like retrieval-augmented generative pipelines and state-of-the-art search systems. It integrates with Hugging Face Transformers, Elasticsearch, OpenSearch, OpenAI, Cohere, Anthropic and others, making it an extremely popular framework for teams of all sizes.\n",
    "\n",
    "https://langfuse.com/docs/integrations/haystack/example-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2fed27",
   "metadata": {},
   "source": [
    "### Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cd54fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install haystack, langfuse, and the langfuse-haystack integration package\n",
    "%pip install haystack-ai langfuse-haystack \"langfuse<3.0.0\"\n",
    "\n",
    "# additional requirements for this cookbook\n",
    "%pip install sentence-transformers datasets mwparserfromhell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "743647bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-962190cc-b2bd-47c0-b752-8de287a2a5c1\" \n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-4e9959c3-0935-4142-b789-734beb81d15a\" \n",
    "LANGFUSE_HOST=\"http://localhost:3000\"\n",
    "\n",
    "# Your openai key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "# Enable Haystack content tracing\n",
    "os.environ[\"HAYSTACK_CONTENT_TRACING_ENABLED\"] = \"True\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645f90b5",
   "metadata": {},
   "source": [
    "### Basic RAG Pipeline\n",
    "Building a basic retrieval-augmented generative (RAG) pipeline. First you’ll load your data to the Document Store, then connect components together into a RAG pipeline, and finally ask a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44942798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from haystack import Document, Pipeline\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder, SentenceTransformersTextEmbedder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.retrievers import InMemoryEmbeddingRetriever\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack_integrations.components.connectors.langfuse import LangfuseConnector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c6ab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline(document_store: InMemoryDocumentStore):\n",
    "    retriever = InMemoryEmbeddingRetriever(document_store=document_store, top_k=2)\n",
    " \n",
    "    # A prompt corresponds to an NLP task and contains instructions for the model. Here, the pipeline will go through each Document to figure out the answer.\n",
    "    template = \"\"\"\n",
    "    Given the following information, answer the question.\n",
    "    Context:\n",
    "    {% for document in documents %}\n",
    "        {{ document.content }}\n",
    "    {% endfor %}\n",
    "    Question: {{question}}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    " \n",
    "    prompt_builder = PromptBuilder(template=template)\n",
    " \n",
    "    basic_rag_pipeline = Pipeline()\n",
    "    # Add components to your pipeline\n",
    "    basic_rag_pipeline.add_component(\"tracer\", LangfuseConnector(\"Basic RAG Pipeline\"))\n",
    "    basic_rag_pipeline.add_component(\n",
    "        \"text_embedder\", SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    )\n",
    "    basic_rag_pipeline.add_component(\"retriever\", retriever)\n",
    "    basic_rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "    # 当前问题: 使用 OpenRouter API key 调用 OpenAI 官方 API\n",
    "    #basic_rag_pipeline.add_component(\"llm\", OpenAIGenerator(model=\"gpt-4o\", generation_kwargs={\"n\": 2}))\n",
    "\n",
    "    # 使用 OpenRouter 兼容的配置\n",
    "    basic_rag_pipeline.add_component(\"llm\", OpenAIGenerator(\n",
    "        api_base_url=\"https://openrouter.ai/api/v1\",\n",
    "        model=\"deepseek/deepseek-chat\",  # 使用 OpenRouter 支持的模型\n",
    "        generation_kwargs={\"n\": 1}  # 减少并发请求\n",
    "    ))\n",
    "\n",
    " \n",
    "    # Now, connect the components to each other\n",
    "    # NOTE: the tracer component doesn't need to be connected to anything in order to work\n",
    "    basic_rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "    basic_rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "    basic_rag_pipeline.connect(\"prompt_builder\", \"llm\")\n",
    " \n",
    "    return basic_rag_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0445490",
   "metadata": {},
   "source": [
    "Then we load data into DocumentStore. In this example, we use the trivia_qa_tiny dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6a78f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store = InMemoryDocumentStore()\n",
    "dataset = load_dataset(\"SpeedOfMagic/trivia_qa_tiny\", split=\"train\")\n",
    "embedder = SentenceTransformersDocumentEmbedder(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "embedder.warm_up()\n",
    " \n",
    "docs_with_embeddings = []\n",
    "for entry in dataset:\n",
    "    # Create a Document object for each entry, handling the question (str) and answer (str) data correctly\n",
    "    content = f\"Question: {entry['question']} Answer: {entry['answer']}\"\n",
    "    doc = Document(content=content)\n",
    " \n",
    "    # Embed the document using the embedder\n",
    "    # Only takes in list of Documents\n",
    "    embedder.run([doc])\n",
    " \n",
    "    # Collect the embedded documents\n",
    "    docs_with_embeddings.append(doc)\n",
    " \n",
    "# Write the embedded documents to the document store\n",
    "document_store.write_documents(docs_with_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac72485",
   "metadata": {},
   "source": [
    "Then ask a question based on the data we loaded in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95522c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = get_pipeline(document_store)\n",
    "question = \"What can you tell me about Truman Capote?\"\n",
    "response = pipeline.run({\"text_embedder\": {\"text\": question}, \"prompt_builder\": {\"question\": question}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be2c30cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.draw(\"./haystack_pipeline.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac2a6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Trace url:\", response[\"tracer\"][\"trace_url\"])\n",
    "print(\"Response:\", response[\"llm\"][\"replies\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070bde85",
   "metadata": {},
   "source": [
    "### Add score to the trace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7033fc",
   "metadata": {},
   "source": [
    "\n",
    "当前环境安装的是 langfuse 2.x，所以无法导入和使用 get_client。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51e61a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langfuse.client.StatefulClient at 0x1a6dc2a7440>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from langfuse import get_client\n",
    " \n",
    "# langfuse = get_client()\n",
    " \n",
    "# trace_id = langfuse.get_current_trace_id()\n",
    " \n",
    "# langfuse.create_score(\n",
    "#     trace_id=trace_id,\n",
    "#     name=\"quality\",\n",
    "#     value=1,\n",
    "#     comment=\"Cordial and relevant\", # optional\n",
    "# )\n",
    "\n",
    "\n",
    "from langfuse import Langfuse\n",
    "\n",
    "langfuse = Langfuse(\n",
    "    public_key=os.environ[\"LANGFUSE_PUBLIC_KEY\"],\n",
    "    secret_key=os.environ[\"LANGFUSE_SECRET_KEY\"],\n",
    "    host=os.environ.get(\"LANGFUSE_HOST\", \"http://localhost:3000\")\n",
    ")\n",
    "\n",
    "# 从 response 获取 trace_id\n",
    "trace_url = response[\"tracer\"][\"trace_url\"]\n",
    "trace_id = trace_url.split(\"/traces/\")[-1].split(\"?\")[0]\n",
    "\n",
    "\n",
    "langfuse.score(\n",
    "    trace_id=trace_id,\n",
    "    name=\"quality\",\n",
    "    value=1,\n",
    "    comment=\"Cordial and relevant\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
