{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09d34783",
   "metadata": {},
   "source": [
    "### Deepseek model with Langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623ed5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop-in replacement to get full logging by changing only the import\n",
    "from langfuse.openai import OpenAI\n",
    "\n",
    "# Configure the OpenAI client to use https://openrouter.ai/api/v1 as base url\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=\"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\",\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek/deepseek-chat\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is Langfuse?\"}],\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69cb76ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"**Langfuse** is an open-source **observability and analytics platform** designed for **LLM (Large Language Model) applications**. It helps developers and teams track, analyze, and optimize interactions with AI models like OpenAI's GPT, Anthropic's Claude, or custom LLMs.\\n\\n### **Key Features of Langfuse:**\\n1. **Tracing & Logging**  \\n   - Records detailed traces of LLM calls (inputs, outputs, latency, tokens, costs).  \\n   - Supports nested tracing for complex workflows (e.g., multi-step AI agents).  \\n\\n2. **Analytics & Dashboards**  \\n   - Visualizes usage metrics (e.g., response times, token consumption, costs).  \\n   - Tracks user feedback (thumbs up/down) and custom metrics.  \\n\\n3. **Prompt Management & Versioning**  \\n   - Stores and versions prompts to compare performance across changes.  \\n   - A/B tests different prompts or model configurations.  \\n\\n4. **Quality Monitoring**  \\n   - Detects anomalies (e.g., unexpected outputs, high latency).  \\n   - Integrates with evaluation frameworks (e.g., LangSmith, custom evals).  \\n\\n5. **Open Source & Self-Hostable**  \\n   - Free to use with a **MIT license**.  \\n   - Can be deployed on your infrastructure (Docker, Kubernetes).  \\n   - Also offers a managed cloud version.  \\n\\n### **Use Cases:**\\n- Debugging LLM applications in development.  \\n- Monitoring production AI deployments.  \\n- Optimizing costs and performance of LLM calls.  \\n\\n### **Integrations:**\\nWorks with **LangChain, LlamaIndex, OpenAI, Anthropic, and custom LLMs**. SDKs are available for **Python & JS/TS**.  \\n\\nüîó **Website**: [langfuse.com](https://langfuse.com)  \\nüìÇ **GitHub**: [github.com/langfuse/langfuse](https://github.com/langfuse/langfuse)  \\n\\nWould you like help setting it up or exploring specific features?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langfuse import observe\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=\"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\"\n",
    ")\n",
    "\n",
    "@observe()\n",
    "def story():\n",
    "    return (\n",
    "        client.chat.completions.create(\n",
    "            model=\"deepseek/deepseek-chat\",\n",
    "            messages=[{\"role\": \"user\", \"content\": \"What is Langfuse?\"}],\n",
    "        )\n",
    "        .choices[0]\n",
    "        .message.content\n",
    "    )\n",
    "\n",
    "@observe()\n",
    "def main():\n",
    "    return story()\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed13b9c0",
   "metadata": {},
   "source": [
    "### Evaluate Langfuse LLM Traces with an External Evaluation Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b7f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langfuse openai deepeval --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e228546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-962190cc-b2bd-47c0-b752-8de287a2a5c1\" \n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-4e9959c3-0935-4142-b789-734beb81d15a\" \n",
    "LANGFUSE_HOST=\"http://localhost:3000\"\n",
    "\n",
    "# Your openai key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879abf56",
   "metadata": {},
   "source": [
    "Let‚Äôs go ahead and generate a list of topic suggestions that we can later query to our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73509b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    " \n",
    "topic_suggestion = \"\"\" You're a world-class journalist, specialized\n",
    "in figuring out which are the topics that excite people the most.\n",
    "Your task is to give me 50 suggestions for pop-science topics that the generalmo\n",
    "public would love to read about. Make sure topics don't repeat.\n",
    "The output must be a comma-separated list. Generate the list and NOTHING else.\n",
    "The use of numbers is FORBIDDEN.\n",
    "\"\"\"\n",
    " \n",
    "output = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": topic_suggestion\n",
    "        }\n",
    "    ],\n",
    "    model=\"deepseek/deepseek-chat:free\",\n",
    " \n",
    "    temperature=1\n",
    ").choices[0].message.content\n",
    " \n",
    "topics = [item.strip() for item in output.split(\",\")]\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f308d03",
   "metadata": {},
   "source": [
    "we‚Äôll use Langfuse‚Äôs @observe() decorator. This decorator automatically monitors all LLM calls (generations) nested in the function. We‚Äôre also using the langfuse class to label and tag the traces, making it easier to fetch them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255078b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import observe, get_client\n",
    "langfuse = get_client()\n",
    " \n",
    "prompt_template = \"\"\"\n",
    "You're an expert science communicator, able to explain complex topics in an\n",
    "approachable manner. Your task is to respond to the questions of users in an\n",
    "engaging, informative, and friendly way. Stay factual, and refrain from using\n",
    "jargon. Your answer should be 4 sentences at max.\n",
    "Remember, keep it ENGAGING and FUN!\n",
    " \n",
    "Question: {question}\n",
    "\"\"\"\n",
    " \n",
    "@observe()\n",
    "def explain_concept(topic):\n",
    "    langfuse.update_current_trace(\n",
    "        name=f\"Explanation '{topic}'\",\n",
    "        tags=[\"ext_eval_pipelines\"]\n",
    "    )\n",
    "    prompt = prompt_template.format(question=topic)\n",
    " \n",
    " \n",
    "    return client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"deepseek/deepseek-chat:free\",\n",
    " \n",
    "        temperature=0.6\n",
    "    ).choices[0].message.content\n",
    " \n",
    " \n",
    "for topic in topics:\n",
    "    print(f\"Input: Please explain to me {topic.lower()}\")\n",
    "    print(f\"Answer: {explain_concept(topic)} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c35fd75",
   "metadata": {},
   "source": [
    "### 1. Fetch Your Traces\n",
    "\n",
    " We‚Äôll take an incremental approach: first, we‚Äôll fetch the initial 10 traces and evaluate them. After that, we‚Äôll add our scores back into Langfuse and move on to the next batch of 10 traces. We‚Äôll keep this cycle going until we‚Äôve processed a total of 50 traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8180cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import get_client\n",
    "from datetime import datetime, timedelta\n",
    " \n",
    "BATCH_SIZE = 10\n",
    "TOTAL_TRACES = 50\n",
    " \n",
    "langfuse = get_client()\n",
    " \n",
    "now = datetime.now()\n",
    "five_am_today = datetime(now.year, now.month, now.day, 5, 0)\n",
    "five_am_yesterday = five_am_today - timedelta(days=1)\n",
    " \n",
    "traces_batch = langfuse.api.trace.list(page=1,\n",
    "                                     limit=BATCH_SIZE,\n",
    "                                     tags=\"ext_eval_pipelines\",\n",
    "                                     from_timestamp=five_am_yesterday,\n",
    "                                     to_timestamp=datetime.now()\n",
    "                                   ).data\n",
    " \n",
    "print(f\"Traces in first batch: {len(traces_batch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76349133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content:  Ah, decision-making‚Äîour brain‚Äôs daily game of ‚Äúchoose your own adventure‚Äù! It‚Äôs a mix of logic, emotions, and even a pinch of gut feeling, all working together to guide us. Sometimes, we overthink (hello, analysis paralysis!), while other times, we go with snap judgments (thanks, instincts!). Understanding this process helps us make better choices and maybe even outsmart our own biases! üß†‚ú®\n"
     ]
    }
   ],
   "source": [
    "print(\"content: \", traces_batch[1].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe9db0f",
   "metadata": {},
   "source": [
    "### 2. Run your evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0768bb",
   "metadata": {},
   "source": [
    "#### 2.1. Categoric Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb36c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_tone_eval = \"\"\"\n",
    "You're an expert in human emotional intelligence. You can identify with ease the\n",
    " tone in human-written text. Your task is to identify the tones present in a\n",
    " piece of <text/> with precission. Your output is a comma separated list of three\n",
    " tones. PRINT THE LIST ALONE, NOTHING ELSE.\n",
    " \n",
    "<possible_tones>\n",
    "neutral, confident, joyful, optimistic, friendly, urgent, analytical, respectful\n",
    "</possible_tones>\n",
    " \n",
    "<example_1>\n",
    "Input: Citizen science plays a crucial role in research by involving everyday\n",
    "people in scientific projects. This collaboration allows researchers to collect\n",
    "vast amounts of data that would be impossible to gather on their own. Citizen\n",
    "scientists contribute valuable observations and insights that can lead to new\n",
    "discoveries and advancements in various fields. By participating in citizen\n",
    "science projects, individuals can actively contribute to scientific research\n",
    "and make a meaningful impact on our understanding of the world around us.\n",
    " \n",
    "Output: respectful,optimistic,confident\n",
    "</example_1>\n",
    " \n",
    "<example_2>\n",
    "Input: Bionics is a field that combines biology and engineering to create\n",
    "devices that can enhance human abilities. By merging humans and machines,\n",
    "bionics aims to improve quality of life for individuals with disabilities\n",
    "or enhance performance for others. These technologies often mimic natural\n",
    "processes in the body to create seamless integration. Overall, bionics holds\n",
    "great potential for revolutionizing healthcare and technology in the future.\n",
    " \n",
    "Output: optimistic,confident,analytical\n",
    "</example_2>\n",
    " \n",
    "<example_3>\n",
    "Input: Social media can have both positive and negative impacts on mental\n",
    "health. On the positive side, it can help people connect, share experiences,\n",
    "and find support. However, excessive use of social media can also lead to\n",
    "feelings of inadequacy, loneliness, and anxiety. It's important to find a\n",
    "balance and be mindful of how social media affects your mental well-being.\n",
    "Remember, it's okay to take breaks and prioritize your mental health.\n",
    " \n",
    "Output: friendly,neutral,respectful\n",
    "</example_3>\n",
    " \n",
    "<text>\n",
    "{text}\n",
    "</text>\n",
    "\"\"\"\n",
    " \n",
    " \n",
    "test_tone_score = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": template_tone_eval.format(\n",
    "                text=traces_batch[1].output),\n",
    "        }\n",
    "    ],\n",
    "    model=\"deepseek/deepseek-chat:free\",\n",
    " \n",
    "    temperature=0\n",
    ").choices[0].message.content\n",
    "print(f\"User query: {traces_batch[1].input['args'][0]}\")\n",
    "print(f\"Model answer: {traces_batch[1].output}\")\n",
    "print(f\"Dominant tones: {test_tone_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a69a5b0",
   "metadata": {},
   "source": [
    "Identifying human intents and tones can be tricky for language models. To handle this, we used a multi-shot prompt, which means giving the model several examples to learn from. Now let‚Äôs wrap our code in an evaluation function for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0513b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tone_score(trace):\n",
    "    return client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": template_tone_eval.format(text=trace.output),\n",
    "            }\n",
    "        ],\n",
    "        model=\"deepseek/deepseek-chat:free\",\n",
    "        temperature=0\n",
    "    ).choices[0].message.content\n",
    " \n",
    "tone_score(traces_batch[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aba712",
   "metadata": {},
   "source": [
    "#### 2.2. Numeric Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "973d2f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: ipywidgets in e:\\conda\\envs\\langfuse\\lib\\site-packages (8.1.7)\n",
      "Requirement already satisfied: comm>=0.1.3 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from ipywidgets) (9.4.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: colorama in e:\\conda\\envs\\langfuse\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in e:\\conda\\envs\\langfuse\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in e:\\conda\\envs\\langfuse\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in e:\\conda\\envs\\langfuse\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in e:\\conda\\envs\\langfuse\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in e:\\conda\\envs\\langfuse\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: executing>=1.2.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in e:\\conda\\envs\\langfuse\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in e:\\conda\\envs\\langfuse\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1c3444",
   "metadata": {},
   "source": [
    "GEval ÊîØÊåÅÁöÑÊ®°Âûã: ÈªòËÆ§‰ΩøÁî® OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51f26d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams, LLMTestCase\n",
    " \n",
    "def joyfulness_score(trace):\n",
    "\t\tjoyfulness_metric = GEval(\n",
    "\t\t    name=\"Correctness\",\n",
    "\t\t    criteria=\"Determine whether the output is engaging and fun.\",\n",
    "\t\t    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "\t\t)\n",
    "\t\ttest_case = LLMTestCase(\n",
    "    input=trace.input[\"args\"],\n",
    "    actual_output=trace.output)\n",
    " \n",
    "\t\tjoyfulness_metric.measure(test_case)\n",
    " \n",
    "\t\tprint(f\"Score: {joyfulness_metric.score}\")\n",
    "\t\tprint(f\"Reason: {joyfulness_metric.reason}\")\n",
    " \n",
    "\t\treturn {\"score\": joyfulness_metric.score, \"reason\": joyfulness_metric.reason}\n",
    " \n",
    "joyfulness_score(traces_batch[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ad8bca",
   "metadata": {},
   "source": [
    "OpenRouter Âíå DeepseekÔºå‰øÆÊîπÊàê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bc3290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.models import DeepEvalBaseLLM\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams, LLMTestCase\n",
    "import os\n",
    "\n",
    "class CustomLLM(DeepEvalBaseLLM):\n",
    "    def __init__(self, model_name=\"deepseek/deepseek-chat\"):\n",
    "        self.model_name = model_name\n",
    "        self.client = OpenAI(\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "            api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    "        )\n",
    "    \n",
    "    def load_model(self):\n",
    "        return self.model_name\n",
    "    \n",
    "    def generate(self, prompt: str) -> str:\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "    \n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        return self.generate(prompt)\n",
    "    \n",
    "    def get_model_name(self):\n",
    "        return self.model_name\n",
    "\n",
    "# ‰ΩøÁî®Ëá™ÂÆö‰πâÊ®°ÂûãËøõË°åËØÑ‰º∞\n",
    "def joyfulness_score(trace):\n",
    "    custom_llm = CustomLLM(\"deepseek/deepseek-chat\")\n",
    "    \n",
    "    joyfulness_metric = GEval(\n",
    "        name=\"Joyfulness\",\n",
    "        criteria=\"Determine whether the output is engaging and fun.\",\n",
    "        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "        model=custom_llm\n",
    "    )\n",
    "    \n",
    "    test_case = LLMTestCase(\n",
    "        input=trace.input[\"args\"],\n",
    "        actual_output=trace.output\n",
    "    )\n",
    "    \n",
    "    joyfulness_metric.measure(test_case)\n",
    "    \n",
    "    return {\"score\": joyfulness_metric.score, \"reason\": joyfulness_metric.reason}\n",
    "\n",
    "joyfulness_score(traces_batch[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dda385",
   "metadata": {},
   "source": [
    " GEval uses chain of thought (CoT) prompting to formulate a set of criteria for scoring prompts. When developing your own metrics, it‚Äôs important to review the reasoning behind these scores. This helps ensure that the model evaluates the traces just as you intended when you wrote the evaluation prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d538dd",
   "metadata": {},
   "source": [
    "### 3. Pushing Scores to Langfuse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4964135b",
   "metadata": {},
   "source": [
    "Now that we have our evaluation functions ready, it‚Äôs time to put them to work. Use the Langfuse client to add scores to existing traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7634f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse.create_score(\n",
    "    trace_id=traces_batch[1].id,\n",
    "    name=\"tone\",\n",
    "    value=joyfulness_score(traces_batch[1])[\"score\"],\n",
    "    comment=joyfulness_score(traces_batch[1])[\"reason\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba48ad3",
   "metadata": {},
   "source": [
    "And thus, you‚Äôve added your first externally-evaluated score to Langfuse! Just 49 more to go üòÅ. But don‚Äôt worry ‚Äî our solutions are easy to scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c314ea9",
   "metadata": {},
   "source": [
    "### 4. Putting everything together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43d3b178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 processed üöÄ \n",
      "\n",
      "Batch 2 processed üöÄ \n",
      "\n",
      "Batch 3 processed üöÄ \n",
      "\n",
      "Batch 4 processed üöÄ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    " \n",
    "for page_number in range(1, math.ceil(TOTAL_TRACES/BATCH_SIZE)):\n",
    " \n",
    "    traces_batch = langfuse.api.trace.list(\n",
    "        tags=\"ext_eval_pipelines\",\n",
    "        page=page_number,\n",
    "        from_timestamp=five_am_yesterday,\n",
    "        to_timestamp=five_am_today,\n",
    "        limit=BATCH_SIZE\n",
    "    ).data\n",
    " \n",
    "    for trace in traces_batch:\n",
    "        print(f\"Processing {trace.name}\")\n",
    " \n",
    "        if trace.output is None:\n",
    "            print(f\"Warning: \\n Trace {trace.name} had no generated output, \\\n",
    "            it was skipped\")\n",
    "            continue\n",
    " \n",
    "        langfuse.create_score(\n",
    "            trace_id=trace.id,\n",
    "            name=\"tone\",\n",
    "            value=tone_score(trace)\n",
    "        )\n",
    " \n",
    "        jscore = joyfulness_score(trace)\n",
    "        langfuse.create_score(\n",
    "            trace_id=trace.id,\n",
    "            name=\"joyfulness\",\n",
    "            value=jscore[\"score\"],\n",
    "            comment=jscore[\"reason\"]\n",
    "        )\n",
    " \n",
    "    print(f\"Batch {page_number} processed üöÄ \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
